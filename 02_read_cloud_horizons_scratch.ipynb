{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Frameworks/Python.framework/Versions/3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\n",
      "1.2.3\n"
     ]
    }
   ],
   "source": [
    "### Install dependencies and import them\n",
    "from time import time\n",
    "!pip3 -q install ipytree~=0.2.1\n",
    "!pip3 -q install matplotlib~=3.3.4\n",
    "\n",
    "!pip3 -q install numpy~=1.19.2\n",
    "!pip3 -q install python-dotenv~=0.15.0\n",
    "!pip3 -q install s3fs~=0.5.2\n",
    "!pip3 -q install zarr~=2.6.1\n",
    "#!pip3 install s3fs\n",
    "!pip3 -q install fsspec\n",
    "!pip3 -q install seaborn\n",
    "!pip3 -q install welly\n",
    "!pip3 -q install real-simple-seismic\n",
    "!pip3 -q install segyio\n",
    "!pip3 -q install pylops\n",
    "!pip3 -q install scikit-learn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "import segyio\n",
    "import pylops\n",
    "from sklearn.linear_model import LinearRegression, RANSACRegressor\n",
    "import numpy as np\n",
    "from rss.client import rssFromS3\n",
    "#import openvds\n",
    "\"\"\"\n",
    "from vds_utils import (\n",
    "    get_minicube,\n",
    "    get_slice,\n",
    "    print_channel_info,\n",
    "    print_crs_metadata,\n",
    ")\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "from os import getenv\n",
    "#!pip install pandas==1.2.0\n",
    "#!pip3 install --upgrade pandas\n",
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up AWS Credentials\n",
    "S3 buckets requires an access key and an access secret, and since nobody wants to\n",
    "include a key and a secret in a notebook, we used `python-dotenv` to keep this\n",
    "in a `.env` file together with the scripts. If the `.env` file is configured correctly\n",
    "the next cell will load that into the current notebook kernel and create the necessary\n",
    "parameter dictionary for pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "region_name = 'us-east-1'\n",
    "#aws_key = getenv(aws_key)\n",
    "#aws_secret = getenv(aws_secret)\n",
    "aws_key = getenv('AKIAQJQE57C5E2EDNTPP')\n",
    "aws_secret = getenv('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')\n",
    "\n",
    "\n",
    "s3_path_wb = 's3://geophysics-on-cloud/poseidon/horizons/water_bottom.csv.gz'\n",
    "s3_path_top_hw = 's3://geophysics-on-cloud/poseidon/horizons/top_heywood.csv.gz'\n",
    "s3_path_top_jam = 's3://geophysics-on-cloud/poseidon/horizons/top_jameison.csv.gz'\n",
    "s3_path_top_joh = 's3://geophysics-on-cloud/poseidon/horizons/top_johnson.csv.gz'\n",
    "s3_path_top_nearpl = 's3://geophysics-on-cloud/poseidon/horizons/top_near_plover.csv.gz'\n",
    "\n",
    "s3_options = {\n",
    "    'client_kwargs': {\n",
    "        'aws_access_key_id': aws_key,\n",
    "        'aws_secret_access_key': aws_secret,\n",
    "        'region_name': region_name,\n",
    "        }\n",
    "    }\n",
    "s3_options = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Cloud Horizons into Pandas DataFrame\n",
    "With the location and configuration above, now we can use the built in `pandas.read_json()`\n",
    "function to download and deserialize the `JSON` file into a `pandas.DataFrame()`.\n",
    "The file gets downloaded in a few seconds. Parsing the `JSON` takes longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed read in 18.603348970413208 seconds\n"
     ]
    }
   ],
   "source": [
    "def get_horizon(s3_path, s3_options):\n",
    "    horizon_data = pd.read_csv(filepath_or_buffer=s3_path, compression='gzip', storage_options=s3_options,)\n",
    "    \n",
    "    horizon_data.set_index(['inline', 'xline'], inplace=True)\n",
    "\n",
    "    print(f\"Completed read in {time() - start_time} seconds\")\n",
    "    \n",
    "    return horizon_data\n",
    "\n",
    "start_time = time()\n",
    "horizon_data_wb = get_horizon(s3_path_wb, s3_options)\n",
    "\n",
    "start_time = time()\n",
    "horizon_data_top_hw = get_horizon(s3_path_top_hw, s3_options)\n",
    "\n",
    "start_time = time()\n",
    "horizon_data_top_jam = get_horizon(s3_path_top_jam, s3_options)\n",
    "\n",
    "start_time = time()\n",
    "horizon_data_top_nearpl = get_horizon(s3_path_top_nearpl, s3_options)\n",
    "\n",
    "start_time = time()\n",
    "horizon_data_top_joh = get_horizon(s3_path_top_joh, s3_options)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "### Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "horizon_data_top_jam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "horizon_data = pd.concat([horizon_data_wb, horizon_data_top_hw, horizon_data_top_jam, horizon_data_top_joh, horizon_data_top_nearpl], axis=1, join=\"inner\")\n",
    "horizon_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "horizon_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for horizon in horizon_data.columns:\n",
    "    plt.figure()\n",
    "    plt.suptitle(horizon)\n",
    "    hrz = horizon_data[horizon]\n",
    "    plt.tripcolor(\n",
    "        hrz.index.get_level_values(0)[::50],\n",
    "        hrz.index.get_level_values(1)[::50],\n",
    "        hrz.values[::50],\n",
    "    )\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and QC wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = 's3://geophysics-on-cloud/poseidon/wells/poseidon_geoml_training_wells.json.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the location and configuration above, now we can use the built in `pandas.read_json()`\n",
    "function to download and deserialize the `JSON` file into a `pandas.DataFrame()`.\n",
    "The file gets downloaded in a few seconds. Parsing the `JSON` takes longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "well_data = pd.read_json(\n",
    "    path_or_buf=s3_path,\n",
    "    compression='gzip',\n",
    "    storage_options=s3_options,\n",
    ")\n",
    "\n",
    "well_data.set_index(['well_id', 'twt'], inplace=True)\n",
    "\n",
    "print(f\"Completed read in {time() - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "### Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "well_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "well_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Wells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for well in well_data.index.levels[0]:\n",
    "    fig, ax = plt.subplots(1, 5, sharey='all',figsize=(10,5))\n",
    "    curves = well_data.loc[well][['inline','tvdss','dtc', 'dts', 'rhob']]\n",
    "    fig.suptitle(well)\n",
    "    for idx, curve in enumerate(curves.columns):\n",
    "        ax[idx].plot(curves[curve], curves[curve].index)\n",
    "        ax[idx].set_ylabel(curves[curve].index.name)\n",
    "        ax[idx].set_xlabel(curves[curve].name)\n",
    "\n",
    "    ax[0].invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the number of wells\n",
    "no_of_well = well_data.index.get_level_values('well_id').unique()\n",
    "print(no_of_well)\n",
    "print(len(no_of_well))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 6 wells - where are they located?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "sns.pairplot(well_data[['dtc', 'dts', 'rhob']]) #,height=3, aspect=.8, kind=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get coeffs of linear fit - dts vs dtc\n",
    "\n",
    "tmp_well = well_data[['dtc', 'dts']].dropna()\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(tmp_well['dtc'],tmp_well['dts'])\n",
    "\n",
    "# use line_kws to set line label for legend\n",
    "ax = sns.regplot(x=\"dtc\", y=\"dts\", data=tmp_well, color='b', \n",
    " line_kws={'label':\"y={0:.5f}x+{1:.5f}\".format(slope,intercept)})\n",
    "\n",
    "# plot legend\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Linear regression fit using sns\n",
    "well_data['dts_calw'] = 0*well_data['dtc']\n",
    "for well in well_data.index.levels[0]:\n",
    "    fig, ax = plt.subplots(1, 1, sharey='all',figsize=(10,5))\n",
    "    tmp_well = well_data.loc[well][['dtc', 'dts']]\n",
    "    curves = tmp_well[['dtc', 'dts']].dropna()\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(curves['dtc'],curves['dts'])\n",
    "    \n",
    "    well_data.loc[well]['dts_calw'] = slope*well_data.loc[well]['dtc'] + intercept\n",
    "    \n",
    "    fig.suptitle(well)\n",
    "    ax = sns.regplot(x=\"dtc\", y=\"dts\", data=curves, color='b', \n",
    "      line_kws={'label':\"y={0:.5f}x+{1:.5f}\".format(slope,intercept)})\n",
    "    # plot legend\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Linear regression fit using sklearn and a robust linear fit with RANSAC\n",
    "### Linear regression and its robust one dtc vs dts - individual wells\n",
    "ransac = RANSACRegressor()\n",
    "lmodel = LinearRegression(fit_intercept=True)\n",
    "\n",
    "well_data['dts_calw'] = 0*well_data['dtc']\n",
    "well_data['dts_calw_r'] = 0*well_data['dtc']\n",
    "for well in well_data.index.levels[0]:\n",
    "    fig, ax = plt.subplots(1, 1, sharey='all',figsize=(10,5))\n",
    "    tmp_well = well_data.loc[well][['dtc', 'dts']]\n",
    "    curves = tmp_well[['dtc', 'dts']].dropna()\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(curves['dtc'],curves['dts'])\n",
    "    \n",
    "    \n",
    "    dtc_arr = np.reshape(curves['dtc'].values, (len(curves['dtc'].values), 1))\n",
    "    lmodel.fit(dtc_arr, curves['dts'].values)\n",
    "    ransac.fit(dtc_arr, curves['dts'].values)\n",
    "    well_data.loc[well]['dts_calw'] = slope*well_data.loc[well]['dtc'] + intercept\n",
    "    well_data.loc[well]['dts_calw_r'] = ransac.estimator_.coef_*well_data.loc[well]['dtc'] + ransac.estimator_.intercept_\n",
    "    \n",
    "    #fig.suptitle(well)\n",
    "    #ax = sns.regplot(x=\"dtc\", y=\"rhob\", data=curves, color='b', \n",
    "    #  line_kws={'label':\"y={0:.5f}x+{1:.5f}\".format(slope,intercept)})\n",
    "    # plot legend\n",
    "    #ax.legend()\n",
    "    line_X = dtc_arr\n",
    "    line_y = lmodel.predict(line_X)\n",
    "    line_y_ransac = ransac.predict(line_X)\n",
    "\n",
    "    #ax.figure(figsize=(2,10))\n",
    "    ax.scatter(line_X, curves['dts'].values)\n",
    "    ax.plot(line_X, line_y, 'r');\n",
    "    ax.plot(line_X, line_y_ransac, 'g');\n",
    "    plt.title(well)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comparing the linear fits with the original log - DTS\n",
    "for well in well_data.index.levels[0]:\n",
    "    fig, ax = plt.subplots(1, 1, sharey='all',figsize=(5,5))\n",
    "    curves = well_data.loc[well][['dts', 'dts_calw','dts_calw_r']]\n",
    "    fig.suptitle(well)\n",
    "    #for idx, curve in enumerate(curves.columns):\n",
    "    ax.plot(curves['dts'], curves['dts'].index, 'g')\n",
    "    ax.plot(curves['dts_calw'], curves['dts_calw'].index, 'b')\n",
    "    ax.plot(curves['dts_calw_r'], curves['dts_calw_r'].index, 'r')\n",
    "    ax.set_ylabel(curves['dts_calw'].index.name)\n",
    "    ax.set_xlabel(curves['dts_calw'].name)\n",
    "    #plt.ylim((3000,4000))\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Linear regression and its robust one dtc vs rhos - individual wells\n",
    "ransac = RANSACRegressor()\n",
    "lmodel = LinearRegression(fit_intercept=True)\n",
    "\n",
    "well_data['rhob_calw'] = 0*well_data['dtc']\n",
    "well_data['rhob_calw_r'] = 0*well_data['dtc']\n",
    "for well in well_data.index.levels[0]:\n",
    "    fig, ax = plt.subplots(1, 1, sharey='all',figsize=(10,5))\n",
    "    tmp_well = well_data.loc[well][['dtc', 'rhob']]\n",
    "    curves = tmp_well[['dtc', 'rhob']].dropna()\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(curves['dtc'],curves['rhob'])\n",
    "    \n",
    "    \n",
    "    dtc_arr = np.reshape(curves['dtc'].values, (len(curves['dtc'].values), 1))\n",
    "    lmodel.fit(dtc_arr, curves['rhob'].values)\n",
    "    ransac.fit(dtc_arr, curves['rhob'].values)\n",
    "    well_data.loc[well]['rhob_calw'] = slope*well_data.loc[well]['dtc'] + intercept\n",
    "    well_data.loc[well]['rhob_calw_r'] = ransac.estimator_.coef_*well_data.loc[well]['dtc'] + ransac.estimator_.intercept_\n",
    "    \n",
    "    #fig.suptitle(well)\n",
    "    #ax = sns.regplot(x=\"dtc\", y=\"rhob\", data=curves, color='b', \n",
    "    #  line_kws={'label':\"y={0:.5f}x+{1:.5f}\".format(slope,intercept)})\n",
    "    # plot legend\n",
    "    #ax.legend()\n",
    "    line_X = dtc_arr\n",
    "    line_y = lmodel.predict(line_X)\n",
    "    line_y_ransac = ransac.predict(line_X)\n",
    "\n",
    "    #ax.figure(figsize=(2,10))\n",
    "    ax.scatter(line_X, curves['rhob'].values)\n",
    "    ax.plot(line_X, line_y, 'r');\n",
    "    ax.plot(line_X, line_y_ransac, 'g');\n",
    "    plt.title(well)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comparing the linear fits with the original log - rhob\n",
    "#well_data['dts_cal'] = slope*well_data['dtc'] + intercept\n",
    "\n",
    "for well in well_data.index.levels[0]:\n",
    "    fig, ax = plt.subplots(1, 1, sharey='all',figsize=(5,5))\n",
    "    curves = well_data.loc[well][['rhob', 'rhob_calw','rhob_calw_r']]\n",
    "    fig.suptitle(well)\n",
    "    #for idx, curve in enumerate(curves.columns):\n",
    "    ax.plot(curves['rhob'], curves['rhob'].index, 'g')\n",
    "    ax.plot(curves['rhob_calw'], curves['rhob_calw'].index, 'b')\n",
    "    ax.plot(curves['rhob_calw_r'], curves['rhob_calw_r'].index, 'r')\n",
    "    ax.set_ylabel(curves['rhob_calw'].index.name)\n",
    "    ax.set_xlabel(curves['rhob_calw'].name)\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "well_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## rhob_new and dts_new - are the new to be rhob and dts logs to be used for training \n",
    "## after the nans\n",
    "well_data['dts_new'] = well_data.dts\n",
    "well_data['rhob_new'] = well_data.rhob\n",
    "\n",
    "# Replace nans in dts from pseudo dts\n",
    "well_data.dts_new.fillna(well_data.dts_calw, inplace=True)\n",
    "well_data.rhob_new.fillna(well_data.rhob_calw_r, inplace=True)\n",
    "\n",
    "## Impedance\n",
    "well_data['imp_dtc_new'] = np.diff(1000000*well_data['rhob_new']/well_data['dtc'], prepend=np.nan)\n",
    "well_data['imp_dts_new'] = np.diff(1000000*well_data['rhob_new']/well_data['dts_new'], prepend=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### QC logs after replacing NaNs\n",
    "## For dts\n",
    "for well in well_data.index.levels[0]:\n",
    "    fig, ax = plt.subplots(1, 3, sharey='all',figsize=(5,5))\n",
    "    curves = well_data.loc[well][['dts', 'dts_new', 'imp_dtc_new', 'imp_dts_new']]\n",
    "    fig.suptitle(well)\n",
    "    #for idx, curve in enumerate(curves.columns):\n",
    "    ax[0].plot(curves['dts_new'], curves['dts_new'].index, 'b')\n",
    "    ax[0].plot(curves['dts'], curves['dts'].index, 'g')\n",
    "    #ax[0].plot(curves['dts_calw_r'], curves['dts_calw_r'].index, 'r')\n",
    "    ax[0].set_ylabel(curves['dts_new'].index.name)\n",
    "    ax[0].set_xlabel(curves['dts_new'].name)\n",
    "    #plt.ylim((3000,4000))\n",
    "    ax[1].plot(curves['imp_dtc_new'], curves['imp_dtc_new'].index, 'b')\n",
    "    #ax[1].plot(curves['dts'], curves['dts'].index, 'g')\n",
    "    #ax[1].plot(curves['dts_calw_r'], curves['dts_calw_r'].index, 'r')\n",
    "    ax[1].set_ylabel(curves['imp_dtc_new'].index.name)\n",
    "    ax[1].set_xlabel(curves['imp_dtc_new'].name)\n",
    "    \n",
    "    ax[2].plot(curves['imp_dts_new'], curves['imp_dts_new'].index, 'b')\n",
    "    #ax[2].plot(curves['dts'], curves['dts'].index, 'g')\n",
    "    #ax[2].plot(curves['dts_calw_r'], curves['dts_calw_r'].index, 'r')\n",
    "    ax[2].set_ylabel(curves['imp_dts_new'].index.name)\n",
    "    ax[2].set_xlabel(curves['imp_dts_new'].name)\n",
    "    \n",
    "    ax[0].invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### QC logs after replacing NaNs\n",
    "## For rhob\n",
    "for well in well_data.index.levels[0]:\n",
    "    fig, ax = plt.subplots(1, 1, sharey='all',figsize=(5,5))\n",
    "    curves = well_data.loc[well][['rhob', 'rhob_new']]\n",
    "    fig.suptitle(well)\n",
    "    #for idx, curve in enumerate(curves.columns):\n",
    "    ax.plot(curves['rhob_new'], curves['rhob_new'].index, 'b')\n",
    "    ax.plot(curves['rhob'], curves['rhob'].index, 'g')\n",
    "    #ax.plot(curves['dts_calw_r'], curves['dts_calw_r'].index, 'r')\n",
    "    ax.set_ylabel(curves['rhob_new'].index.name)\n",
    "    ax.set_xlabel(curves['rhob_new'].name)\n",
    "    #plt.ylim((3000,4000))\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "bin_edges = [2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0]\n",
    "for well in well_data.index.levels[0]:\n",
    "    fig, ax = plt.subplots(1, 1, sharey='all',figsize=(5,5))\n",
    "    curves = well_data.loc[well][['rhob', 'rhob_calw','rhob_calw_r']]\n",
    "    fig.suptitle(well)\n",
    "    #for idx, curve in enumerate(curves.columns):\n",
    "    ax.hist(curves['rhob'], bins=bin_edges, ls='dashed', lw=3, fc=(0, 0, 1, 0.5)) #, curves['rhob'].index, 'g')\n",
    "    ax.hist(curves['rhob_calw'], bins=bin_edges, ls='dotted', lw=3, fc=(1, 0, 0, 0.5)) #, curves['rhob_calw'].index, 'b')\n",
    "    ax.hist(curves['rhob_calw_r'], bins=bin_edges, lw=3, fc=(0, 0, 0, 0.5)) #, curves['rhob_calw_r'].index, 'r')\n",
    "    ax.set_ylabel('frequency')\n",
    "    ax.set_xlabel(curves['rhob_calw'].name)\n",
    "    #plt.ylim((3000,4000))\n",
    "    #ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot the dtc vs rhob - individual wells - maybe prep dtc well\n",
    "## extract the horizon layer from the well data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_well = well_data[['dtc', 'dts']].dropna()\n",
    "tmp_well.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QC Seismic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rss.client import rssFromS3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "#aws_key = getenv('AKIAQJQE57C5E2EDNTPP')\n",
    "#aws_secret = getenv('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')\n",
    "\n",
    "\n",
    "client_kwargs = {'aws_access_key_id': 'AKIAQJQE57C5E2EDNTPP',\n",
    "                 'aws_secret_access_key': '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'}\n",
    "\n",
    "rss = rssFromS3('s3://geophysics-on-cloud/poseidon/seismic/rss/psdn11_TbsdmF_full_w_AGC_Nov11', client_kwargs)\n",
    "rss_n = rssFromS3('s3://geophysics-on-cloud/poseidon/seismic/rss/psdn11_TbsdmF_Near_Nov_11_32bit', client_kwargs)\n",
    "rss_m = rssFromS3('s3://geophysics-on-cloud/poseidon/seismic/rss/psdn11_TbsdmF_Mid_Nov_11_32bit', client_kwargs)\n",
    "rss_f = rssFromS3('s3://geophysics-on-cloud/poseidon/seismic/rss/psdn11_TbsdmF_Far_Nov_11_32bit', client_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Header information\n",
    "The rss object contains the header information, x/y (easting and northing) and the \n",
    "il/xl coordinates. If you're interested, the CRS is GDA95, which is EPSG:28355. This \n",
    "data is for each trace in the file, in the order the trace occured in the original \n",
    "SEGY data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x/y header data, with coordinate scalar applied\n",
    "rss.xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inline/xline header data\n",
    "rss.ilxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading individual traces\n",
    "You can load an individual trace by it's inline/xline coordinate. Understand that loading a \n",
    "single trace will also being down an entire line from storage, so it won't be faster than \n",
    "loading a line. However, neighbouring traces will be cached locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "st = time()\n",
    "trace, is_live = rss.trace(1337, 2000)\n",
    "print (\"Elapsed Time (loading) : \", time() - st)\n",
    "\n",
    "st = time()\n",
    "trace, is_live = rss.trace(1337, 2000)\n",
    "print (\"Elapsed Time (caching) : \", time() - st)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(trace, np.arange(len(trace)))\n",
    "plt.ylim([len(trace),0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting slices:\n",
    "The data is optimized for reads in the inline and crossline directions, \n",
    "and reads are cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st = time()\n",
    "inline, mask = rss.line(2002, sort_order='inline')\n",
    "print (\"Elapsed Time (loading) : \", time() - st)\n",
    "\n",
    "st = time()\n",
    "inline, mask = rss.line(2003, sort_order='inline')\n",
    "print (\"Elapsed Time (loading): \", time() - st)\n",
    "\n",
    "st = time()\n",
    "inline, mask = rss.line(2002, sort_order='inline')\n",
    "print (\"Elapsed Time (caching): \", time() - st)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(inline, cmap='gray', \n",
    "           interpolation='bicubic',\n",
    "           aspect=1.5,\n",
    "           vmin=-2*np.std(inline[~mask]), vmax=2*np.std(inline[~mask]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "st = time()\n",
    "inline, mask = rss.line(1100, sort_order='crossline')\n",
    "print (\"Elapsed Time (loading): \", time() - st)\n",
    "\n",
    "st = time()\n",
    "inline, mask = rss.line(1101, sort_order='crossline')\n",
    "print (\"Elapsed Time (loading): \", time() - st)\n",
    "\n",
    "st = time()\n",
    "inline, mask = rss.line(1100, sort_order='crossline')\n",
    "print (\"Elapsed Time (caching): \", time() - st)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(inline, cmap='gray', \n",
    "           aspect=1.5, \n",
    "           interpolation='bicubic',\n",
    "           vmin=-2*np.std(inline[~mask]), vmax=2*np.std(inline[~mask]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map x/y -> inline/xline\n",
    "This is really slow the first time you run it, and probably uses a bunch of \n",
    "memory, but gives you a function to find nearest neighbours to a specific x/y\n",
    "location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "st = time()\n",
    "rss.query_by_xy(rss.xy[1337,:])\n",
    "print (\"Elapsed time (loading): \", time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "st = time()\n",
    "dist, ilxl = rss.query_by_xy(rss.xy[1337,:], k=8)\n",
    "print (\"Elapsed time (cached) : \", time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the distance in meters from the x/y location to the k nearest grid locations:\n",
    "print(dist)\n",
    "# the inline/xline coordinates\n",
    "print(ilxl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an array of traces around a point\n",
    "st = time()\n",
    "traces = np.vstack([rss.trace(*i)[0] for i in ilxl[0]]).T\n",
    "print (\"Elapsed Time (loading) : \", time() - st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 8, sharey='all',figsize=(20,5))\n",
    "fig.suptitle('nearby traces')\n",
    "for idx in range(traces.shape[1]):\n",
    "    ax[idx].plot(traces[:,idx], np.arange(len(traces[:,idx])))\n",
    "    ax[idx].set_ylabel('twt')\n",
    "    ax[idx].set_xlabel('amplitude')\n",
    "\n",
    "ax[0].invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute cross-corelation of the extracted traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 8, sharey='all',figsize=(20,5))\n",
    "fig.suptitle('nearby traces')\n",
    "for idx in range(traces.shape[1]):\n",
    "    #ax[idx].plot(traces[:,idx], np.arange(len(traces[:,idx])))\n",
    "    ax[idx].xcorr(traces[:,0], traces[:,idx], usevlines=True, maxlags=50, normed=True, lw=2)\n",
    "    ax[idx].set_xlabel('twt')\n",
    "    ax[idx].set_ylabel('amplitude')\n",
    "\n",
    "#ax[0].invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Check the cross-corelation of the near-by well\n",
    "def get_xcorr_trace_nearby(traces):\n",
    "    xcor_trc = []\n",
    "    for idx in range(traces.shape[1]):\n",
    "        corr_norm = np.max(np.correlate(traces[:,0], traces[:,0]))\n",
    "        tmp = np.max(np.correlate(traces[:,0], traces[:,idx])/corr_norm )\n",
    "        xcor_trc.append(tmp)\n",
    "        \n",
    "    return xcor_trc\n",
    "\n",
    "xcorr_max = get_xcorr_trace_nearby(traces)\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(xcorr_max)), xcorr_max)\n",
    "plt.xlim([len(xcorr_max),0])\n",
    "plt.title('correlation values of near-by traces to the picked location')\n",
    "plt.ylabel('correlation values')\n",
    "plt.xlabel('trace numbers')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rss.trace(1337, 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to get a quick estimate of the wavelet in our data using a simple *statistical wavelet* estimation in frequency domain.\n",
    "\n",
    "Note that this notebook is not focused on the pre-processing but we will need access to this to apply a relative seismic inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = inline.T\n",
    "d = d[:2500]\n",
    "dt=4 ## might need to changes this\n",
    "nt_wav = 31 # lenght of wavelet in samples\n",
    "nfft = 2**11 # lenght of fft\n",
    "\n",
    "def get_wav_state(d, st, et):\n",
    "    \n",
    "    # time axis for wavelet\n",
    "    t_wav = np.arange(nt_wav) * (dt/1000) \n",
    "    t_wav = np.concatenate((np.flipud(-t_wav[1:]), t_wav), axis=0)\n",
    "\n",
    "    # estimate wavelet spectrum\n",
    "    wav_est_fft = np.mean(np.abs(np.fft.fft(d[..., st:et], nfft, axis=-1)), axis=0)\n",
    "    fwest = np.fft.fftfreq(nfft, d=dt/1000)\n",
    "\n",
    "    # create wavelet in time\n",
    "    wav_est = np.real(np.fft.ifft(wav_est_fft)[:nt_wav])\n",
    "    wav_est = np.concatenate((np.flipud(wav_est[1:]), wav_est), axis=0)\n",
    "    wav_est = wav_est / wav_est.max()\n",
    "    wcenter = np.argmax(np.abs(wav_est))\n",
    "\n",
    "    return fwest, wav_est_fft, t_wav, wav_est\n",
    "\n",
    "fwest_sh, wav_est_fft_sh, t_wav_sh, wav_est_sh = get_wav_state(d, 200, 400)\n",
    "fwest_md, wav_est_fft_md, t_wav_md, wav_est_md = get_wav_state(d, 500, 700)\n",
    "fwest_dp, wav_est_fft_dp, t_wav_dp, wav_est_dp = get_wav_state(d, 1000, 1200)\n",
    "\n",
    "# display wavelet\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "fig.suptitle('Statistical wavelet estimate')\n",
    "axs[0].plot(fwest_sh[:nfft//2], wav_est_fft_sh[:nfft//2], 'k')\n",
    "axs[0].plot(fwest_md[:nfft//2], wav_est_fft_md[:nfft//2], 'r')\n",
    "axs[0].plot(fwest_dp[:nfft//2], wav_est_fft_dp[:nfft//2], 'b')\n",
    "axs[0].set_title('Frequency')\n",
    "axs[1].plot(t_wav_sh, wav_est_sh, 'k')\n",
    "axs[1].plot(t_wav_md, wav_est_md, 'r')\n",
    "axs[1].plot(t_wav_dp, wav_est_dp, 'b')\n",
    "axs[1].set_title('Time');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get wavelet from welllog - \n",
    "## Redo wavelet from target formation - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss.xy[1337,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dt = 4\n",
    "def get_trace_around(rss, num, ntrc):\n",
    "    dist, ilxl = rss.query_by_xy(rss.xy[num,:], k=ntrc)\n",
    "    traces = np.vstack([rss.trace(*i)[0] for i in ilxl[0]]).T\n",
    "    return traces\n",
    "\n",
    "trace_from_1200 =  get_trace_around(rss, 1200, 25)\n",
    "\n",
    "fig, ax = plt.subplots(1, 8, sharey='all',figsize=(20,5))\n",
    "fig.suptitle('nearby traces')\n",
    "idx = 0\n",
    "for i in range(1, trace_from_1200.shape[1], 3):\n",
    "    ax[idx].plot(trace_from_1200[:,i], dt*np.arange(len(trace_from_1200[:,i])))\n",
    "    ax[idx].set_ylabel('twt')\n",
    "    ax[idx].set_xlabel('amplitude')\n",
    "    idx += 1\n",
    "\n",
    "ax[0].invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = trace_from_1200.T\n",
    "fwest_sh, wav_est_fft_sh, t_wav_sh, wav_est_sh = get_wav_state(d, 200, 400)\n",
    "fwest_md, wav_est_fft_md, t_wav_md, wav_est_md = get_wav_state(d, 500, 700)\n",
    "fwest_dp, wav_est_fft_dp, t_wav_dp, wav_est_dp = get_wav_state(d, 1000, 1200)\n",
    "\n",
    "# display wavelet\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "fig.suptitle('Statistical wavelet estimate')\n",
    "axs[0].plot(fwest_sh[:nfft//2], wav_est_fft_sh[:nfft//2], 'k')\n",
    "axs[0].plot(fwest_md[:nfft//2], wav_est_fft_md[:nfft//2], 'r')\n",
    "axs[0].plot(fwest_dp[:nfft//2], wav_est_fft_dp[:nfft//2], 'b')\n",
    "axs[0].set_title('Frequency')\n",
    "axs[1].plot(t_wav_sh, wav_est_sh, 'k')\n",
    "axs[1].plot(t_wav_md, wav_est_md, 'r')\n",
    "axs[1].plot(t_wav_dp, wav_est_dp, 'b')\n",
    "axs[1].set_title('Time');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "from scipy.fftpack import fft, fftshift\n",
    "window = signal.tukey(201)\n",
    "\n",
    "print(d.shape, window.shape)\n",
    "wind_ext = np.pad(window, (int((d.shape[1]-len(window))/2), ),  'constant', constant_values=0)\n",
    "d_win = np.zeros(d.shape)\n",
    "for i in range(d.shape[0]):\n",
    "    d_win[i, :] = np.multiply(d[i, :],wind_ext) #np.convolve(d[i, :],wind_ext, 'same')\n",
    "\n",
    "print(d_win.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time_seismic = dt*np.arange(0, d.shape[1])\n",
    "fig, ax = plt.subplots(1, 2, sharey='all',figsize=(2,15))\n",
    "ax[0].imshow(d_win.T, cmap='gray', aspect='auto', extent=[0, d_win.shape[0], time_seismic[1], time_seismic[-1]])\n",
    "ax[1].imshow(d.T, cmap='gray', aspect='auto', extent=[0, d_win.shape[0], time_seismic[1], time_seismic[-1]])\n",
    "\n",
    "ax[0].invert_yaxis()\n",
    "#ax[1].invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look into the near, mid, far and maybe full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trace_full_1200 =  get_trace_around(rss, 1200, 25)\n",
    "trace_near_1200 =  get_trace_around(rss_n, 1200, 25)\n",
    "trace_mid_1200 =  get_trace_around(rss_m, 1200, 25)\n",
    "trace_far_1200 =  get_trace_around(rss_f, 1200, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shallow - \n",
    "d = trace_from_1200.T\n",
    "fwest_fl, wav_est_fft_fl, t_wav_fl, wav_est_fl = get_wav_state(trace_full_1200.T, 200, 400)\n",
    "fwest_n, wav_est_fft_n, t_wav_n, wav_est_n = get_wav_state(trace_near_1200.T, 200, 400)\n",
    "fwest_m, wav_est_fft_m, t_wav_m, wav_est_m = get_wav_state(trace_mid_1200.T, 200, 400)\n",
    "fwest_f, wav_est_fft_f, t_wav_f, wav_est_f = get_wav_state(trace_far_1200.T, 200, 400)\n",
    "\n",
    "# display wavelet\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "fig.suptitle('Statistical wavelet estimate')\n",
    "axs[0].plot(fwest_n[:nfft//2], wav_est_fft_n[:nfft//2], 'k')\n",
    "axs[0].plot(fwest_m[:nfft//2], wav_est_fft_m[:nfft//2], 'r')\n",
    "axs[0].plot(fwest_f[:nfft//2], wav_est_fft_f[:nfft//2], 'b')\n",
    "axs[0].set_title('Frequency')\n",
    "axs[1].plot(t_wav_n, wav_est_n, 'k')\n",
    "axs[1].plot(t_wav_m, wav_est_m, 'r')\n",
    "axs[1].plot(t_wav_f, wav_est_f, 'b')\n",
    "axs[1].set_title('Time');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mid depth - \n",
    "d = trace_from_1200.T\n",
    "fwest_fl, wav_est_fft_fl, t_wav_fl, wav_est_fl = get_wav_state(trace_full_1200.T, 500, 700)\n",
    "fwest_n, wav_est_fft_n, t_wav_n, wav_est_n = get_wav_state(trace_near_1200.T, 500, 700)\n",
    "fwest_m, wav_est_fft_m, t_wav_m, wav_est_m = get_wav_state(trace_mid_1200.T, 500, 700)\n",
    "fwest_f, wav_est_fft_f, t_wav_f, wav_est_f = get_wav_state(trace_far_1200.T, 500, 700)\n",
    "\n",
    "# display wavelet\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "fig.suptitle('Statistical wavelet estimate')\n",
    "axs[0].plot(fwest_n[:nfft//2], wav_est_fft_n[:nfft//2], 'k')\n",
    "axs[0].plot(fwest_m[:nfft//2], wav_est_fft_m[:nfft//2], 'r')\n",
    "axs[0].plot(fwest_f[:nfft//2], wav_est_fft_f[:nfft//2], 'b')\n",
    "axs[0].set_title('Frequency')\n",
    "axs[1].plot(t_wav_n, wav_est_n, 'k')\n",
    "axs[1].plot(t_wav_m, wav_est_m, 'r')\n",
    "axs[1].plot(t_wav_f, wav_est_f, 'b')\n",
    "axs[1].set_title('Time');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## deep depth - \n",
    "d = trace_from_1200.T\n",
    "fwest_fl, wav_est_fft_fl, t_wav_fl, wav_est_fl = get_wav_state(trace_full_1200.T, 1000, 1200)\n",
    "fwest_n, wav_est_fft_n, t_wav_n, wav_est_n = get_wav_state(trace_near_1200.T, 1000, 1200)\n",
    "fwest_m, wav_est_fft_m, t_wav_m, wav_est_m = get_wav_state(trace_mid_1200.T, 1000, 1200)\n",
    "fwest_f, wav_est_fft_f, t_wav_f, wav_est_f = get_wav_state(trace_far_1200.T, 1000, 1200)\n",
    "\n",
    "# display wavelet\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "fig.suptitle('Statistical wavelet estimate')\n",
    "axs[0].plot(fwest_n[:nfft//2], wav_est_fft_n[:nfft//2], 'k')\n",
    "axs[0].plot(fwest_m[:nfft//2], wav_est_fft_m[:nfft//2], 'r')\n",
    "axs[0].plot(fwest_f[:nfft//2], wav_est_fft_f[:nfft//2], 'b')\n",
    "axs[0].set_title('Frequency')\n",
    "axs[1].plot(t_wav_n, wav_est_n, 'k')\n",
    "axs[1].plot(t_wav_m, wav_est_m, 'r')\n",
    "axs[1].plot(t_wav_f, wav_est_f, 'b')\n",
    "axs[1].set_title('Time');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "def prep_wavelet_4_log(t_wav, wav_est):\n",
    "    time=t_wav\n",
    "    dt_log = 0.0005\n",
    "    time_log = np.linspace(time[0], time[-1], int((time[-1]-time[0])/dt_log) + 1)\n",
    "    print(time[-1]-time[0], len(time_log), len(time))\n",
    "    wav_est_log = signal.resample(wav_est, len(time_log))\n",
    "    #wav_est_log = signal.resample_poly(wav_est, len(time_log)*10, int(len(time_log)/10))\n",
    "    \n",
    "    #wav_est_log = resample(wav_est, n_samples=len(time_log))\n",
    "    \n",
    "    \n",
    "    wav_est_fft = np.abs(np.fft.fft(wav_est, nfft, axis=-1)) #, axis=0)\n",
    "    wav_est_log_fft = np.abs(np.fft.fft(wav_est_log, nfft, axis=-1)) #, axis=0)\n",
    "    fwest_log = np.fft.fftfreq(nfft, d=dt_log)\n",
    "    fwest = np.fft.fftfreq(nfft, d=4/1000)\n",
    "    \n",
    "    return time_log, wav_est_log, wav_est_fft, wav_est_log_fft, fwest,fwest_log \n",
    "\n",
    "time_sh_log, wav_est_sh_log, wav_est_fft, wav_est_log_fft, fwest,fwest_log = prep_wavelet_4_log(t_wav_sh, wav_est_sh)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(time_sh_log, wav_est_sh_log)\n",
    "plt.plot(t_wav_sh, wav_est_sh)\n",
    "#plt.xlim([len(xcorr_max),0])\n",
    "plt.show()\n",
    "\n",
    "print(len(fwest))\n",
    "\n",
    "# display wavelet\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 5))\n",
    "fig.suptitle('Statistical wavelet estimate')\n",
    "axs[0].plot(fwest[:nfft//2], wav_est_fft[:nfft//2], 'k')\n",
    "axs[0].plot(fwest_log[:nfft//2], wav_est_log_fft[:nfft//2], 'r')\n",
    "#axs[0].plot(fwest_dp[:nfft//2], wav_est_fft_dp[:nfft//2], 'b')\n",
    "axs[0].set_title('Frequency')\n",
    "axs[1].plot(t_wav_sh, wav_est_sh, 'k')\n",
    "axs[1].plot(time_sh_log, wav_est_sh_log, 'r')\n",
    "#axs[1].plot(t_wav_dp, wav_est_dp, 'b')\n",
    "axs[1].set_title('Time');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###  get well synthetic\n",
    "\n",
    "#Need to get the well locations\n",
    "\n",
    "for well in well_data.index.levels[0]:\n",
    "    fig, ax = plt.subplots(1, 3, sharey='all',figsize=(5,5))\n",
    "    curves = well_data.loc[well][['imp_dtc_new', 'imp_dts_new']]\n",
    "    curves = curves.fillna(0)\n",
    "    time=curves['imp_dts_new'].index\n",
    "    well_syn = np.convolve(wav_est_sh_log, curves['imp_dts_new'], mode='same')\n",
    "    ax[0].plot(curves['imp_dts_new'], curves['imp_dts_new'].index, 'b')\n",
    "    ax[0].set_ylabel(curves['imp_dts_new'].index.name)\n",
    "    ax[0].set_xlabel(curves['imp_dts_new'].name)\n",
    "    \n",
    "    ax[1].plot(well_syn, curves['imp_dts_new'].index, 'b')\n",
    "    ax[1].set_ylabel(curves['imp_dts_new'].index.name)\n",
    "    ax[1].set_xlabel(curves['imp_dts_new'].name)\n",
    "    \n",
    "    \n",
    "    ax[0].invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get the well synthetic QC time interval\n",
    "#well_data\n",
    "\n",
    "for well in well_data.index.levels[0]:\n",
    "    fig, ax = plt.subplots(1, 1, sharey='all',figsize=(5,5))\n",
    "    curves = well_data.loc[well][['dts']]\n",
    "    curves = curves.fillna(0)\n",
    "    time=curves['dts'].index\n",
    "    dt_cuv = np.diff(time, prepend=np.nan)\n",
    "    \n",
    "    ax.plot(dt_cuv, curves['dts'].index, 'b')\n",
    "    ax.set_ylabel(curves['dts'].index.name)\n",
    "    ax.set_xlabel(curves['dts'].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss.query_by_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ilxl = rss.ilxl\n",
    "print(ilxl.shape, np.min(ilxl[:, 0]),  np.max(ilxl[:, 0]),  np.min(ilxl[:, 1]), np.max(ilxl[:, 1]))\n",
    "il_range = np.max(ilxl[:, 0]) -  np.min(ilxl[:, 0]) + 1\n",
    "xl_range = np.max(ilxl[:, 1]) -  np.min(ilxl[:, 1]) + 1\n",
    "ntime = 1501\n",
    "def get_3Ddata(rss, ilxl, il_range, xl_range):\n",
    "    \n",
    "    data_3D = np.zeros((ntime, il_range, xl_range))\n",
    "    for xl in range(np.min(ilxl[0, :]), np.max(ilxl[:, 0])):\n",
    "        #ixline, mask = rss.line(xl, sort_order='crossline')\n",
    "        #data_3D[:, :, xl] = ixline\n",
    "    return data_3D\n",
    "\n",
    "data_d = get_3Ddata(rss, ilxl, il_range, xl_range)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_39_env",
   "language": "python",
   "name": "py_39_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
